{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.7.1, llvm 15.0.7, commit 0f143b2f, osx, python 3.11.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 05/28/24 02:03:18.796 5321176] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=metal\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "import src.edge_constructors as edge\n",
    "import src.clustering as clu\n",
    "import src.metrics_fr as fr\n",
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis Evaluation\n",
    "\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath, n=None):\n",
    "    assert filepath[-4:] == \".csv\", \"Must be a .csv file\"\n",
    "    data = pd.read_csv(filepath)\n",
    "    if n:\n",
    "        data = data.head(n)\n",
    "\n",
    "    attrs = {\n",
    "        \"titles\": data[\"title\"].tolist(),\n",
    "        \"text\": data[\"text\"].tolist(),\n",
    "        \"tags\": data[\"tags\"].apply(ast.literal_eval).tolist(),\n",
    "        \"ids\": data.index.tolist()\n",
    "    }\n",
    "\n",
    "    if \"simplified_tags\" in data.columns:\n",
    "        attrs[\"simplified_tags\"] = data[\"simplified_tags\"].apply(ast.literal_eval).tolist()\n",
    "\n",
    "    return attrs\n",
    "\n",
    "def load_embeddings(dataset_name, model_names):\n",
    "    embeddings = []\n",
    "    for name in model_names:\n",
    "        embeddings.append(utils.load_from_pickle(f\"embeddings/{dataset_name}_{name}_n10000.pickle\"))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: interview_prep.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"interview_prep\"\n",
    "data = load_data(f\"data/{data_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_source</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>agg_method</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric</th>\n",
       "      <th>between_all_nodes</th>\n",
       "      <th>between_shared_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>interview_prep</td>\n",
       "      <td>minilm</td>\n",
       "      <td>mean</td>\n",
       "      <td>cosine</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.684910</td>\n",
       "      <td>0.706979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>interview_prep</td>\n",
       "      <td>minilm</td>\n",
       "      <td>mean</td>\n",
       "      <td>cosine</td>\n",
       "      <td>median</td>\n",
       "      <td>0.674775</td>\n",
       "      <td>0.715765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interview_prep</td>\n",
       "      <td>minilm</td>\n",
       "      <td>mean</td>\n",
       "      <td>cosine</td>\n",
       "      <td>std_dev</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.070315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interview_prep</td>\n",
       "      <td>minilm</td>\n",
       "      <td>mean</td>\n",
       "      <td>soft_cosine</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.641387</td>\n",
       "      <td>0.656642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interview_prep</td>\n",
       "      <td>minilm</td>\n",
       "      <td>mean</td>\n",
       "      <td>soft_cosine</td>\n",
       "      <td>median</td>\n",
       "      <td>0.644162</td>\n",
       "      <td>0.661419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>interview_prep</td>\n",
       "      <td>minilm</td>\n",
       "      <td>mean</td>\n",
       "      <td>soft_cosine</td>\n",
       "      <td>std_dev</td>\n",
       "      <td>0.126290</td>\n",
       "      <td>0.134193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>interview_prep</td>\n",
       "      <td>minilm</td>\n",
       "      <td>mean</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.537721</td>\n",
       "      <td>0.561980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>interview_prep</td>\n",
       "      <td>minilm</td>\n",
       "      <td>mean</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>median</td>\n",
       "      <td>0.521811</td>\n",
       "      <td>0.566406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>interview_prep</td>\n",
       "      <td>minilm</td>\n",
       "      <td>mean</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>std_dev</td>\n",
       "      <td>0.075639</td>\n",
       "      <td>0.077599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>interview_prep</td>\n",
       "      <td>mpnet</td>\n",
       "      <td>mean</td>\n",
       "      <td>cosine</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.707133</td>\n",
       "      <td>0.725702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      data_source embedding_model agg_method  metric_name   metric  \\\n",
       "0  interview_prep          minilm       mean       cosine     mean   \n",
       "1  interview_prep          minilm       mean       cosine   median   \n",
       "2  interview_prep          minilm       mean       cosine  std_dev   \n",
       "3  interview_prep          minilm       mean  soft_cosine     mean   \n",
       "4  interview_prep          minilm       mean  soft_cosine   median   \n",
       "5  interview_prep          minilm       mean  soft_cosine  std_dev   \n",
       "6  interview_prep          minilm       mean    euclidean     mean   \n",
       "7  interview_prep          minilm       mean    euclidean   median   \n",
       "8  interview_prep          minilm       mean    euclidean  std_dev   \n",
       "0  interview_prep           mpnet       mean       cosine     mean   \n",
       "\n",
       "   between_all_nodes  between_shared_tags  \n",
       "0           0.684910             0.706979  \n",
       "1           0.674775             0.715765  \n",
       "2           0.069721             0.070315  \n",
       "3           0.641387             0.656642  \n",
       "4           0.644162             0.661419  \n",
       "5           0.126290             0.134193  \n",
       "6           0.537721             0.561980  \n",
       "7           0.521811             0.566406  \n",
       "8           0.075639             0.077599  \n",
       "0           0.707133             0.725702  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = fr.get_embedding_similarity_metrics_per_dataset(\"interview_prep\", data[\"tags\"],\n",
    "                                             [\"minilm\", \"mpnet\", \"nomic\", \"bert\", \"specter\", \"word2vec\"],\n",
    "                                             [\"mean\"])\n",
    "df1.to_csv(\"analysis/metric1_interview.csv\")\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Purity, Homogeneity, and Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping minilm, mean, kmeans15 due to insufficient samples\n",
      "Skipping minilm, mean, kmeans20 due to insufficient samples\n",
      "Skipping minilm, mean, gmm5 due to insufficient samples\n",
      "Skipping minilm, mean, gmm15 due to insufficient samples\n",
      "Skipping minilm, mean, gmm20 due to insufficient samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikluu/Documents/Poly/Thesis/LociNet/.venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (10) by BIRCH is less than (15). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/Users/erikluu/Documents/Poly/Thesis/LociNet/.venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (10) by BIRCH is less than (20). Decrease the threshold.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping mpnet, mean, kmeans15 due to insufficient samples\n",
      "Skipping mpnet, mean, kmeans20 due to insufficient samples\n"
     ]
    }
   ],
   "source": [
    "k_values = [1, 2, 5, 10, 15, 20]\n",
    "clustering_methods = {\n",
    "    **{f\"kmeans{k}\": lambda x, k=k: clu.kmeans(x, k) for k in k_values},\n",
    "    **{f\"dbscan_eps{eps}_min{min_samples}\": lambda x, eps=eps, min_samples=min_samples: clu.dbscan(x, eps=eps, min_samples=min_samples) for eps in [0.1, 0.3, 0.5, 0.7, 1.0] for min_samples in [3, 5, 10, 15]},\n",
    "    **{f\"gmm{n_components}\": lambda x, n_components=n_components: clu.gmm(x, n_components) for n_components in k_values},\n",
    "    **{f\"birch{k}\": lambda x, k=k: clu.birch(x, k) for k in k_values}\n",
    "}\n",
    "\n",
    "df2 = fr.compare_cluster_metrics(\"interview_prep\",\n",
    "                                    [\"minilm\", \"mpnet\", \"nomic\", \"bert\", \"specter\", \"word2vec\"],\n",
    "                                    [\"mean\"],\n",
    "                                    clustering_methods, data[\"ids\"], data[\"tags\"], k=2)\n",
    "df2.to_csv(\"analysis/metric2_interview.csv\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Assignment Evaluation: Tag Connectivity and Degree of Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 2, 5]\n",
    "clustering_methods = {\n",
    "    **{f\"kmeans{k}\": lambda x, k=k: clu.kmeans(x, k) for k in k_values},\n",
    "    **{f\"dbscan_eps{eps}_min{min_samples}\": lambda x, eps=eps, min_samples=min_samples: clu.dbscan(x, eps=eps, min_samples=min_samples) for eps in [0.1, 0.3, 0.5, 0.7, 1.0] for min_samples in [3, 5, 10, 15]},\n",
    "    **{f\"gmm{n_components}\": lambda x, n_components=n_components: clu.gmm(x, n_components) for n_components in k_values},\n",
    "    **{f\"birch{k}\": lambda x, k=k: clu.birch(x, k) for k in k_values}\n",
    "}\n",
    "\n",
    "edge_connector_methods = {\n",
    "    \"random_edges\": lambda sim_mat, document_ids: edge.random_edges(sim_mat, document_ids, num_edges_per_node=5),\n",
    "    **{f\"knn{k}\": lambda sim_mat, document_ids, k=k: edge.knn(sim_mat, document_ids, k) for k in [3, 5, 10, 15]},\n",
    "    **{f\"knn_mst{k}\": lambda sim_mat, document_ids, k=k: edge.knn_mst(sim_mat, document_ids, k) for k in [3, 5, 10, 15]},\n",
    "    **{f\"threshold_{threshold}\": lambda sim_mat, document_ids, threshold=threshold: edge.threshold_based_edge_assignment(sim_mat, document_ids, threshold) for threshold in [0.3, 0.5, 0.7, 0.9]},\n",
    "    **{f\"mutual_knn{k}\": lambda sim_mat, document_ids, k=k: edge.mutual_knn_edge_assignment(sim_mat, document_ids, k) for k in [3, 5, 10, 15]},\n",
    "    **{f\"spectral_clustering{n_clusters}\": lambda sim_mat, document_ids, n_clusters=n_clusters: edge.spectral_clustering_edge_assignment(sim_mat, document_ids, n_clusters) for n_clusters in [2, 3, 5, 10]}\n",
    "}\n",
    "\n",
    "df3 = fr.compare_edge_assignment_metrics(\"interview_prep\",\n",
    "                                        [\"minilm\", \"mpnet\", \"nomic\", \"bert\", \"specter\", \"word2vec\"],\n",
    "                                        [\"mean\"],\n",
    "                                        clustering_methods,\n",
    "                                        edge_connector_methods, data[\"ids\"], data[\"tags\"], data[\"titles\"], max_depth=3)\n",
    "df3.to_csv(\"analysis/metric3_interview.csv\")\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: Medium (n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"medium_1k_tags_simplified\"\n",
    "data = load_data(f\"data/{data_name}.csv\", n=5000)\n",
    "ids = data[\"ids\"]\n",
    "titles = data[\"titles\"]\n",
    "tags = data[\"simplified_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = fr.get_embedding_similarity_metrics_per_dataset(\"medium1k\", tags,\n",
    "                                             [\"minilm\"],\n",
    "                                             [\"mean\"])\n",
    "df1.to_csv(\"analysis/metric1_medium1k_minilm.csv\")\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = fr.get_embedding_similarity_metrics_per_dataset(\"medium1k\", tags,\n",
    "                                             [\"mpnet\", \"nomic\", \"bert\", \"specter\", \"word2vec\"],\n",
    "                                             [\"mean\"])\n",
    "df1.to_csv(\"analysis/metric1_medium1k_rest.csv\")\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Purity, Homogeneity, and Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 2, 5, 10, 15, 20, 30, 40, 50, 65, 80, 100, 120, 150]\n",
    "\n",
    "clustering_methods = {\n",
    "    **{f\"kmeans{k}\": lambda x, k=k: clu.kmeans(x, k) for k in k_values},\n",
    "    **{f\"dbscan_eps{eps}_min{min_samples}\": lambda x, eps=eps, min_samples=min_samples: clu.dbscan(x, eps=eps, min_samples=min_samples) for eps in [0.1, 0.3, 0.5, 0.7, 1.0] for min_samples in [3, 5, 10, 15]},\n",
    "    **{f\"gmm{n_components}\": lambda x, n_components=n_components: clu.gmm(x, n_components) for n_components in k_values},\n",
    "    **{f\"birch{k}\": lambda x, k=k: clu.birch(x, k) for k in k_values}\n",
    "}\n",
    "\n",
    "df2 = fr.compare_cluster_metrics(\"medium1k\",\n",
    "                                    [\"minilm\", \"mpnet\", \"nomic\", \"bert\", \"specter\", \"word2vec\"],\n",
    "                                    [\"mean\"],\n",
    "                                    clustering_methods,\n",
    "                                    ids, tags, k=2)\n",
    "\n",
    "df2.to_csv(\"analysis/metric2_medium1k.csv\")\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Assignment Evaluation: Tag Connectivity and Degree of Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 2, 5, 10, 15, 20, 30, 40, 50, 65, 80, 100, 120, 150]\n",
    "threshold_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "clustering_methods = {\n",
    "    **{f\"kmeans{k}\": lambda x, k=k: clu.kmeans(x, k) for k in k_values},\n",
    "    **{f\"dbscan_eps{eps}_min{min_samples}\": lambda x, eps=eps, min_samples=min_samples: clu.dbscan(x, eps=eps, min_samples=min_samples) for eps in [0.1, 0.3, 0.5, 0.7, 1.0] for min_samples in [3, 5, 10, 15]},\n",
    "    **{f\"gmm{n_components}\": lambda x, n_components=n_components: clu.gmm(x, n_components) for n_components in k_values},\n",
    "    **{f\"birch{k}\": lambda x, k=k: clu.birch(x, k) for k in k_values}\n",
    "}\n",
    "\n",
    "edge_assignment_methods = {\n",
    "    **{f\"knn{k}\": lambda sim_mat, ids, k=k: edge.knn(sim_mat, ids, k) for k in k_values},\n",
    "    **{f\"knn_mst{k}\": lambda sim_mat, ids, k=k: edge.knn_mst(sim_mat, ids, k) for k in k_values},\n",
    "    **{f\"threshold{threshold}\": lambda sim_mat, ids, threshold=threshold: edge.threshold_based_edge_assignment(sim_mat, ids, threshold) for threshold in threshold_values},\n",
    "    **{f\"mutual_knn{k}\": lambda sim_mat, ids, k=k: edge.mutual_knn_edge_assignment(sim_mat, ids, k) for k in k_values},\n",
    "    **{f\"spectral{n_clusters}\": lambda sim_mat, ids, n_clusters=n_clusters: edge.spectral_clustering_edge_assignment(sim_mat, ids, n_clusters) for n_clusters in k_values}\n",
    "}\n",
    "\n",
    "df3 = fr.compare_edge_assignment_metrics(\"interview_prep\",\n",
    "                                        [\"minilm\", \"mpnet\", \"nomic\", \"bert\", \"specter\", \"word2vec\"],\n",
    "                                        [\"mean\"],\n",
    "                                        clustering_methods,\n",
    "                                        edge_assignment_methods,\n",
    "                                        ids, tags, titles, max_depth=3)\n",
    "\n",
    "df3.to_csv(\"analysis/metric3_medium1k.csv\")\n",
    "print(df3.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     19\u001b[0m k_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m75\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m120\u001b[39m]\n\u001b[1;32m     21\u001b[0m clustering_methods \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkmeans\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, k\u001b[38;5;241m=\u001b[39mk: clu\u001b[38;5;241m.\u001b[39mkmeans(x, k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values},\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbscan_eps\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_min\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, eps\u001b[38;5;241m=\u001b[39meps, min_samples\u001b[38;5;241m=\u001b[39mmin_samples: clu\u001b[38;5;241m.\u001b[39mdbscan(x, eps\u001b[38;5;241m=\u001b[39meps, min_samples\u001b[38;5;241m=\u001b[39mmin_samples) \u001b[38;5;28;01mfor\u001b[39;00m eps \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m1.0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m min_samples \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m15\u001b[39m]},\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgmm\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, n_components\u001b[38;5;241m=\u001b[39mn_components: clu\u001b[38;5;241m.\u001b[39mgmm(x, n_components) \u001b[38;5;28;01mfor\u001b[39;00m n_components \u001b[38;5;129;01min\u001b[39;00m k_values},\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbirch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, k\u001b[38;5;241m=\u001b[39mk: clu\u001b[38;5;241m.\u001b[39mbirch(x, k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values}\n\u001b[1;32m     26\u001b[0m }\n\u001b[0;32m---> 28\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mfr\u001b[49m\u001b[38;5;241m.\u001b[39mcompare_cluster_metrics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedium1k\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m                                     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminilm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     30\u001b[0m                                     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     31\u001b[0m                                     clustering_methods,\n\u001b[1;32m     32\u001b[0m                                     ids, tags, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m     34\u001b[0m df2\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis/metric2_medium1k_minilm.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fr' is not defined"
     ]
    }
   ],
   "source": [
    "# k_values = [1, 2, 5, 10, 15, 20, 30, 40, 50, 65, 80, 100, 120, 150]\n",
    "\n",
    "# clustering_methods = {\n",
    "#     **{f\"kmeans{k}\": lambda x, k=k: clu.kmeans(x, k) for k in k_values},\n",
    "#     **{f\"dbscan_eps{eps}_min{min_samples}\": lambda x, eps=eps, min_samples=min_samples: clu.dbscan(x, eps=eps, min_samples=min_samples) for eps in [0.1, 0.3, 0.5, 0.7, 1.0] for min_samples in [3, 5, 10, 15]},\n",
    "#     **{f\"gmm{n_components}\": lambda x, n_components=n_components: clu.gmm(x, n_components) for n_components in k_values},\n",
    "#     **{f\"birch{k}\": lambda x, k=k: clu.birch(x, k) for k in k_values}\n",
    "# }\n",
    "\n",
    "# df2 = fr.compare_cluster_metrics(\"medium1k\",\n",
    "#                                     [\"minilm\", \"mpnet\", \"nomic\", \"bert\", \"specter\", \"word2vec\"],\n",
    "#                                     [\"mean\"],\n",
    "#                                     clustering_methods,\n",
    "#                                     ids, tags, k=15)\n",
    "\n",
    "# df2.to_csv(\"analysis/metric2_medium1k.csv\")\n",
    "i = 0\n",
    "\n",
    "k_values = [1, 2, 5, 10, 15, 50, 100]\n",
    "\n",
    "clustering_methods = {\n",
    "    **{f\"kmeans{k}\": lambda x, k=k: clu.kmeans(x, k) for k in k_values},\n",
    "    **{f\"dbscan_eps{eps}_min{min_samples}\": lambda x, eps=eps, min_samples=min_samples: clu.dbscan(x, eps=eps, min_samples=min_samples) for eps in [0.1, 0.3, 0.5, 0.7, 1.0] for min_samples in [3, 5, 10, 15]},\n",
    "    **{f\"gmm{n_components}\": lambda x, n_components=n_components: clu.gmm(x, n_components) for n_components in k_values},\n",
    "    **{f\"birch{k}\": lambda x, k=k: clu.birch(x, k) for k in k_values}\n",
    "}\n",
    "\n",
    "df2 = fr.compare_cluster_metrics(\"medium1k\",\n",
    "                                    [\"minilm\"],\n",
    "                                    [\"mean\"],\n",
    "                                    clustering_methods,\n",
    "                                    ids, tags, k=15)\n",
    "\n",
    "df2.to_csv(\"analysis/metric2_medium1k_minilm.csv\")\n",
    "\n",
    "print(i)\n",
    "i+=1\n",
    "\n",
    "df2 = fr.compare_cluster_metrics(\"medium1k\",\n",
    "                                    [\"specter\"],\n",
    "                                    [\"mean\"],\n",
    "                                    clustering_methods,\n",
    "                                    ids, tags, k=15)\n",
    "\n",
    "\n",
    "df2.to_csv(\"analysis/metric2_medium1k_specter.csv\")\n",
    "\n",
    "print(i)\n",
    "i+=1\n",
    "\n",
    "\n",
    "df2 = fr.compare_cluster_metrics(\"medium1k\",\n",
    "                                    [\"word2vec\"],\n",
    "                                    [\"mean\"],\n",
    "                                    clustering_methods,\n",
    "                                    ids, tags, k=15)\n",
    "\n",
    "df2.to_csv(\"analysis/metric2_medium1k_word2vec.csv\")\n",
    "\n",
    "print(i)\n",
    "i+=1\n",
    "\n",
    "\n",
    "df2 = fr.compare_cluster_metrics(\"medium1k\",\n",
    "                                    [\"bert\"],\n",
    "                                    [\"mean\"],\n",
    "                                    clustering_methods,\n",
    "                                    ids, tags, k=15)\n",
    "\n",
    "df2.to_csv(\"analysis/metric2_medium1k_bert.csv\")\n",
    "\n",
    "print(i)\n",
    "i+=1\n",
    "\n",
    "\n",
    "df2 = fr.compare_cluster_metrics(\"medium1k\",\n",
    "                                    [\"nomic\"],\n",
    "                                    [\"mean\"],\n",
    "                                    clustering_methods,\n",
    "                                    ids, tags, k=15)\n",
    "\n",
    "df2.to_csv(\"analysis/metric2_medium1k_nomic.csv\")\n",
    "\n",
    "print(i)\n",
    "i+=1\n",
    "\n",
    "\n",
    "df2 = fr.compare_cluster_metrics(\"medium1k\",\n",
    "                                    [\"mpnet\"],\n",
    "                                    [\"mean\"],\n",
    "                                    clustering_methods,\n",
    "                                    ids, tags, k=15)\n",
    "\n",
    "df2.to_csv(\"analysis/metric2_medium1k_bert.csv\")\n",
    "\n",
    "print(i)\n",
    "i+=1\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "# k_values = [1, 2, 5, 10, 15, 20, 30, 40, 50, 65, 80, 100, 120, 150]\n",
    "# threshold_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# clustering_methods = {\n",
    "#     **{f\"kmeans{k}\": lambda x, k=k: clu.kmeans(x, k) for k in k_values},\n",
    "#     **{f\"dbscan_eps{eps}_min{min_samples}\": lambda x, eps=eps, min_samples=min_samples: clu.dbscan(x, eps=eps, min_samples=min_samples) for eps in [0.1, 0.3, 0.5, 0.7, 1.0] for min_samples in [3, 5, 10, 15]},\n",
    "#     **{f\"gmm{n_components}\": lambda x, n_components=n_components: clu.gmm(x, n_components) for n_components in k_values},\n",
    "#     **{f\"birch{k}\": lambda x, k=k: clu.birch(x, k) for k in k_values}\n",
    "# }\n",
    "\n",
    "# edge_assignment_methods = {\n",
    "#     **{f\"knn{k}\": lambda sim_mat, ids, k=k: edge.knn(sim_mat, ids, k) for k in k_values},\n",
    "#     **{f\"knn_mst{k}\": lambda sim_mat, ids, k=k: edge.knn_mst(sim_mat, ids, k) for k in k_values},\n",
    "#     **{f\"threshold{threshold}\": lambda sim_mat, ids, threshold=threshold: edge.threshold_based_edge_assignment(sim_mat, ids, threshold) for threshold in threshold_values},\n",
    "#     **{f\"mutual_knn{k}\": lambda sim_mat, ids, k=k: edge.mutual_knn_edge_assignment(sim_mat, ids, k) for k in k_values},\n",
    "#     **{f\"spectral{n_clusters}\": lambda sim_mat, ids, n_clusters=n_clusters: edge.spectral_clustering_edge_assignment(sim_mat, ids, n_clusters) for n_clusters in k_values}\n",
    "# }\n",
    "\n",
    "# df3 = fr.compare_edge_assignment_metrics(\"interview_prep\",\n",
    "#                                         [\"minilm\", \"mpnet\", \"nomic\", \"bert\", \"specter\", \"word2vec\"],\n",
    "#                                         [\"mean\"],\n",
    "#                                         clustering_methods,\n",
    "#                                         edge_assignment_methods,\n",
    "#                                         ids, tags, titles, max_depth=8)\n",
    "\n",
    "# df3.to_csv(\"analysis/metric3_medium1k.csv\")\n",
    "# df3.head(10)\n",
    "\n",
    "\n",
    "k_values = [1, 2, 5, 10, 15, 50, 100]\n",
    "threshold_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "clustering_methods = {\n",
    "    **{f\"kmeans{k}\": lambda x, k=k: clu.kmeans(x, k) for k in k_values},\n",
    "    **{f\"dbscan_eps{eps}_min{min_samples}\": lambda x, eps=eps, min_samples=min_samples: clu.dbscan(x, eps=eps, min_samples=min_samples) for eps in [0.1, 0.3, 0.5, 0.7, 1.0] for min_samples in [3, 5, 10, 15]},\n",
    "    **{f\"gmm{n_components}\": lambda x, n_components=n_components: clu.gmm(x, n_components) for n_components in k_values},\n",
    "    **{f\"birch{k}\": lambda x, k=k: clu.birch(x, k) for k in k_values}\n",
    "}\n",
    "\n",
    "edge_assignment_methods = {\n",
    "    **{f\"random{k}\": lambda x, k=k: edge.random_edges(x, ids, k) for k in k_values},\n",
    "    **{f\"knn{k}\": lambda sim_mat, ids, k=k: edge.knn(sim_mat, ids, k) for k in k_values},\n",
    "    **{f\"knn_mst{k}\": lambda sim_mat, ids, k=k: edge.knn_mst(sim_mat, ids, k) for k in k_values},\n",
    "    **{f\"threshold{threshold}\": lambda sim_mat, ids, threshold=threshold: edge.threshold_based_edge_assignment(sim_mat, ids, threshold) for threshold in threshold_values},\n",
    "    **{f\"mutual_knn{k}\": lambda sim_mat, ids, k=k: edge.mutual_knn_edge_assignment(sim_mat, ids, k) for k in k_values},\n",
    "    **{f\"spectral{n_clusters}\": lambda sim_mat, ids, n_clusters=n_clusters: edge.spectral_clustering_edge_assignment(sim_mat, ids, n_clusters) for n_clusters in k_values}\n",
    "}\n",
    "\n",
    "df3 = fr.compare_edge_assignment_metrics(\"interview_prep\",\n",
    "                                        [\"minilm\"],\n",
    "                                        [\"mean\"],\n",
    "                                        clustering_methods,\n",
    "                                        edge_assignment_methods,\n",
    "                                        ids, tags, titles, max_depth=8)\n",
    "\n",
    "df3.to_csv(\"analysis/metric3_medium1k_minilm.csv\")\n",
    "\n",
    "print(i)\n",
    "i+=1\n",
    "\n",
    "\n",
    "df3 = fr.compare_edge_assignment_metrics(\"interview_prep\",\n",
    "                                        [\"specter\"],\n",
    "                                        [\"mean\"],\n",
    "                                        clustering_methods,\n",
    "                                        edge_assignment_methods,\n",
    "                                        ids, tags, titles, max_depth=8)\n",
    "\n",
    "df3.to_csv(\"analysis/metric3_medium1k_specter.csv\")\n",
    "\n",
    "print(i)\n",
    "i+=1\n",
    "\n",
    "\n",
    "df3 = fr.compare_edge_assignment_metrics(\"interview_prep\",\n",
    "                                        [\"word2vec\"],\n",
    "                                        [\"mean\"],\n",
    "                                        clustering_methods,\n",
    "                                        edge_assignment_methods,\n",
    "                                        ids, tags, titles, max_depth=8)\n",
    "\n",
    "df3.to_csv(\"analysis/metric3_medium1k_word2vec.csv\")\n",
    "\n",
    "print(i)\n",
    "i+=1\n",
    "\n",
    "\n",
    "df3 = fr.compare_edge_assignment_metrics(\"interview_prep\",\n",
    "                                        [\"bert\"],\n",
    "                                        [\"mean\"],\n",
    "                                        clustering_methods,\n",
    "                                        edge_assignment_methods,\n",
    "                                        ids, tags, titles, max_depth=8)\n",
    "\n",
    "df3.to_csv(\"analysis/metric3_medium1k_bert.csv\")\n",
    "\n",
    "print(i)\n",
    "i+=1\n",
    "\n",
    "\n",
    "df3 = fr.compare_edge_assignment_metrics(\"interview_prep\",\n",
    "                                        [\"nomic\"],\n",
    "                                        [\"mean\"],\n",
    "                                        clustering_methods,\n",
    "                                        edge_assignment_methods,\n",
    "                                        ids, tags, titles, max_depth=8)\n",
    "\n",
    "df3.to_csv(\"analysis/metric3_medium1k_nomic.csv\")\n",
    "\n",
    "print(i)\n",
    "i+=1\n",
    "\n",
    "\n",
    "df3 = fr.compare_edge_assignment_metrics(\"interview_prep\",\n",
    "                                        [\"mpnet\"],\n",
    "                                        [\"mean\"],\n",
    "                                        clustering_methods,\n",
    "                                        edge_assignment_methods,\n",
    "                                        ids, tags, titles, max_depth=8)\n",
    "\n",
    "df3.to_csv(\"analysis/metric3_medium1k_mpnet.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
